{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Crashlytics"]},{"cell_type":"markdown","metadata":{},"source":["In this notebook we investigate [US Car Accident Data](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents). \n","\n","Our goal is to create a classifier that can predict the severity of a car accident given the other features in the dataset. We will use three methods of classification (Decision Trees, XGBoost, and a Neural Network) and compare the results.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Update to project goal"]},{"cell_type":"markdown","metadata":{},"source":["\n","Severity is defined rather cryptically in this dataset. According to the [original paper](https://arxiv.org/pdf/1906.05409.pdf) detailing the creation of the dataset, crash severity is a native feature of the data, which was collected from both Bing and MapQuest. Bing and MapQuest use different scales for severity, [1,4] for Bing and [0,4] for MapQuest, the similarity between the two ranking systems being that a larger number indicates a more severe accident. The only insight into the meaning of accident severity is that it is typically correlated with duration, distance, and delay where:\n","- **Duration**: how long was traffic impacted by the accident\n","- **Distance**: how far did the impact on traffic extend\n","- **Delay**: how much delay in traffic flow was caused\n","\n","However, these three features are very poorly recorded in the dataset. In the majority of records, crash duration is recorded as either 30 or 45 minutes and crash distance is recorded as 0 miles or approximately 0 miles. Delay is nowhere to be found. So it is impossible to determine whether severity is actually determined by some combination of duration, distance, and delay.\n","\n","Though the original intent of this project was to create a car accident severity predictor that could predict the severity of an accident given the circumstances the accident occured under, the classifiers are actually predicting how either Bing or MapQuest would classify the severity of an accident given the less-than-ideal data those services either generate or receive."]},{"cell_type":"markdown","metadata":{},"source":["## Import/install dependencies, create SparkSession, and read in data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["try:\n","    import pyspark\n","except ModuleNotFoundError:\n","    !pip3 install pyspark\n","    import pyspark\n","try:\n","    import pandas as pd\n","except ModuleNotFoundError:\n","    !pip3 install pandas\n","    import pandas as pd\n","    import csv\n","try:\n","    import matplotlib.pyplot as plt\n","except ModuleNotFoundError:\n","    !pip3 install matplotlib\n","    import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import *\n","from pyspark.ml import Pipeline\n","from pyspark.ml.classification import DecisionTreeClassifier, MultilayerPerceptronClassifier\n","from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, IndexToString, PCA, StandardScaler, Tokenizer, Word2Vec\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n","from pyspark.ml.clustering import KMeans\n","from pyspark.ml.evaluation import ClusteringEvaluator\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["try: \n","    import sklearn\n","except:\n","    !pip3 install scikit-learn\n","    import sklearn\n","try:\n","    from xgboost import XGBClassifier\n","    from xgboost.spark import SparkXGBClassifier\n","except ModuleNotFoundError:\n","    !pip3 install xgboost\n","    from xgboost import XGBClassifier\n","    from xgboost.spark import SparkXGBClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#May need to install java for this to work\n","ss=SparkSession.builder.master(\"local\").appName(\"crashlytics\").getOrCreate()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rawDf = ss.read.csv(\"reduced_crash_data.csv\", header=True, inferSchema=True)\n","#rawDf.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["## Imbalance in Data\n","\n","One important characteristic of our dataset is that a crash severity of 2 or 3 is much more common than any other severity ranking, which we will have to consider when building and evaluating our classifiers."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["severityCount = rawDf.groupBy(F.col(\"Severity\")).count()\n","#severityCount.show()\n","\n","extremeSeverities = rawDf.filter(F.col(\"Severity\") != 3).filter(F.col(\"Severity\") != 2)\n","#extremeSeverities.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Data preprocessing\n","\n","There are a number of steps we need to take to prepare our data for KMeans Clustering and PCA and for use by the various classifiers.\n","\n","First, we will add a crash duration column derived from the start_time and end_time columns to give our various models a data type they can work with (double) rather than a time. This also allows us to drop the start_time and end_time columns."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cleanedDf = rawDf.withColumn(\"Duration\", (F.col(\"End_Time\").cast(\"long\") - F.col(\"Start_Time\").cast(\"long\"))/60)\n","cleanedDf = cleanedDf.drop(\"Start_Time\").drop(\"End_Time\")"]},{"cell_type":"markdown","metadata":{},"source":["We will also adjust the range of severity values from [1,4] to [0,3], as some of the models we will be using require that the label values start at 0."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cleanedDf = cleanedDf.withColumn(\"Severity\", F.col(\"Severity\")-1)"]},{"cell_type":"markdown","metadata":{},"source":["Convert categorical string columns into integer indices so that they can be ingested by the models."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labelIndexer = StringIndexer(inputCols = [\"State\", \"Sunrise_Sunset\"], outputCols = [\"StateId\", \"DaytimeId\"]).fit(rawDf)\n","cleanedDf = labelIndexer.transform(cleanedDf)"]},{"cell_type":"markdown","metadata":{},"source":["We drop non-categorical string columns that cannot be easily indexed and we do not expect to relevant to determining crash severity."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","cleanedDf = cleanedDf.drop(\"ID\", \"Airport_Code\", \"Zipcode\", \"Source\", \"Start_Time\", \"End_Time\", \"End_Lat\", \"End_Lng\", \"City\", \"County\", \"Zipcode\", \"Country\", \"Timezone\", \\\n","                         \"Weather_Timestamp\", \"Wind_Direction\", \"Civil_Twilight\", \"Nautical_Twilight\", \"Astronomical_Twilight\", \"Turning_Loop\", \"State\", \"Street\", \"Sunrise_Sunset\")"]},{"cell_type":"markdown","metadata":{},"source":["For string columns that we may want to keep, we can try using a method to convert the values into vectors that still retain some of the semantic information of the values (by magic). \n","\n","The model we tried using for this conversion is called Word2Vec, but, though it succeeded, it increased the dimensionality of our data drastically. We think the increased dimensionality is too large a price to pay for **potentially** retaining some data about the weather during an accident and the description of the accident, which may not even be useful in our classification models."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vectorizeStrings = False\n","\n","if vectorizeStrings:\n","    descTokenizer = Tokenizer(inputCol=\"Description\", outputCol=\"descWords\")\n","    tokenizedDescDf = descTokenizer.transform(cleanedDf)\n","    descVectorizer = Word2Vec(vectorSize=5, minCount=1, inputCol=\"descWords\", outputCol=\"descWordsVec\")\n","    descModel = descVectorizer.fit(tokenizedDescDf)\n","    vectorizedDescriptionDf = descModel.transform(tokenizedDescDf)\n","\n","    tokenizer_weather = Tokenizer(inputCol=\"Weather_Condition\", outputCol=\"weatherWords\")\n","    vectorizedDescriptionDf = vectorizedDescriptionDf.fillna({\"Weather_Condition\": \"\"})\n","    tokenizedWeatherDf = tokenizer_weather.transform(vectorizedDescriptionDf)\n","    weatherVectorizer = Word2Vec(vectorSize=3, minCount=1, inputCol=\"weatherWords\", outputCol=\"weatherWordsVec\")\n","    weatherModel = weatherVectorizer.fit(tokenizedWeatherDf)\n","    vectorizedWeatherDf = weatherModel.transform(tokenizedWeatherDf)\n","\n","    #wordVecData_weather.select(\"Description\", \"desc_word2vec\", \"Weather_Condition\", \"weather_word2vec\").show(420,truncate=False)\n","\n","    cleanedDf = vectorizedWeatherDf.drop(\"Description\", \"Weather_Condition\", \"desc_words\", \"weather_words\")\n","\n","else:\n","    cleanedDf = cleanedDf.drop(\"Description\", \"Weather_Condition\")"]},{"cell_type":"markdown","metadata":{},"source":["Some of the columns in the dataset have lots of null values, so we replace any null values in those columns with 0. We believe that for these columns 0 is a fairly obvious stand-in value â€“ if the wind chill, precipitation, or wind speed were not recorded they were likely not important factors in the accident."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cleanedDf = cleanedDf.fillna({\"Wind_Chill(F)\": 0, \"Precipitation(in)\": 0, \"Wind_Speed(mph)\": 0})"]},{"cell_type":"markdown","metadata":{},"source":["Our dataframe cleanedDf is now ready for use in KMeans, PCA, and classification."]},{"cell_type":"markdown","metadata":{},"source":["## Vectorize our feature columns for use in K-Means clustering and PCA\n","\n","The rest of the processing we do on our data prefers our table be condensed into a single vectorized column, so we do that here.\n","\n","For K-Means and PCA we will retain severity in the features column, later for the classifier we will drop that information from the features column."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Ignore reamining null values\n","kmeansAssembler = VectorAssembler(inputCols=cleanedDf.columns, outputCol=\"features\", handleInvalid=\"skip\")\n","kmeansData = kmeansAssembler.transform(cleanedDf)\n","\n","lostRecordCount = cleanedDf.count() - kmeansData.count()\n","print(\"Lost records due to null values: \", lostRecordCount) \n","print(\"Fraction of total records lost: \", lostRecordCount/cleanedDf.count())"]},{"cell_type":"markdown","metadata":{},"source":["## K-Means Clustering\n","\n","This data has a large number of features, so we will perform K-Means Clustering to see if we can find some patterns in our data and PCA to better understand which features of our dataset affect crash severity."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def fit_kmeans(df_input,num_cluster_centers=3):\n","  kmeans = KMeans().setK(num_cluster_centers).setSeed(1)\n","  \n","  model = kmeans.fit(df_input)\n","  \n","  clustered_data = model.transform(df_input)\n","\n","  wcss = model.summary.trainingCost\n","  \n","  return clustered_data, wcss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kValues = range(2, 20)\n","wcss_scores = []\n","\n","for k in kValues:\n","  clustered_data, wcss = fit_kmeans(kmeansData,num_cluster_centers=k)\n","  wcss_scores.append(wcss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(kValues, wcss_scores, marker='o')\n","plt.title(\"Elbow Method for Optimal K\")\n","plt.xlabel(\"Number of Clusters (K)\")\n","plt.ylabel(\"WCSS Scores\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["After performing K-Means clustering with 2-20 clusters, there is no obvious candidate for optimal number of clusters. This could indicate that our data does not have well-separated, distinct clusters. Reducing the dimensionality of our data may lead to better results, so after completing PCA and identifying the principle components of our dataset we will revisit K-Means."]},{"cell_type":"markdown","metadata":{},"source":["## PCA\n","\n","This data has a large number of features, so we will perform PCA to see what the true dimensionality of our data is, and see if we can produce better K-Means results after reducing our data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n","scalerModel = scaler.fit(kmeansData)\n","scaledData = scalerModel.transform(kmeansData)\n","\n","pca = PCA(k=18, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n","pcaModel = pca.fit(scaledData)\n","result = pcaModel.transform(scaledData)\n","\n","print(\"Explained Variance: \", sum(pcaModel.explainedVariance))"]},{"cell_type":"markdown","metadata":{},"source":["We will find the minimum number of principle components needed to explain at least 95% of the variance. We want to reduce the dimensionality of our data as much as possible but still retain a high amount of the variance."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["principleComponents = range(1, 26)\n","bestPcCount = 1\n","for pc in principleComponents:\n","    pca=PCA(k=pc, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n","    pcaModel = pca.fit(scaledData)\n","    if sum(pcaModel.explainedVariance) > 0.95:\n","        bestPcCount = pc\n","        break\n","bestPca = PCA(k=bestPcCount, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n","bestPcaModel = bestPca.fit(scaledData)\n","pcaReducedDf = bestPcaModel.transform(scaledData)\n","\n","print(\"Princple Components needed for >95% explained variance: \", bestPcCount)\n","print(\"Explained Variance: \", sum(bestPcaModel.explainedVariance))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def fit_kmeans_column(df_input,column_name='pcaFeatures',num_cluster_centers=3):\n","  kmeans = KMeans(featuresCol=column_name).setK(num_cluster_centers).setSeed(1)\n","  \n","  model = kmeans.fit(df_input)\n","  \n","  clustered_data = model.transform(df_input)\n","\n","  wcss = model.summary.trainingCost\n","  \n","  return clustered_data, wcss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wcss_scores_pca = []\n","\n","for k in kValues:\n","  clustered_data, wcss = fit_kmeans_column(pcaReducedDf,column_name=\"pcaFeatures\",num_cluster_centers=k)\n","  wcss_scores_pca.append(wcss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(kValues, wcss_scores_pca, marker='o')\n","plt.title(\"Elbow Method for Optimal K\")\n","plt.xlabel(\"Number of Clusters (K)\")\n","plt.ylabel(\"WCSS Scores\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Unfortunately the results of K-Means clustering are still inconclusive after performing PCA and extracting the principle components of our dataset. Moreover, we were not able to greatly reduce the dimensionality of our data by performing PCA, in order to explain 95% of the variance in our data we needed to retain 21 principle components from 26 features."]},{"cell_type":"markdown","metadata":{},"source":["## Vectorize our feature columns for use by classifiers\n","\n","For the classifier we will drop severity from the features column, as that will be the label column that the classifier will be trying to predict.\n","\n","We also split our data into a training set and test set, using 75% of the data in training and reserving 25% for testing. As we will be using these two sets for each of the classifiers, we cache them using the persist() command."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifierAssembler = VectorAssembler(inputCols=cleanedDf.drop(\"Severity\").columns, outputCol=\"features\", handleInvalid=\"skip\")\n","classifierData = classifierAssembler.transform(cleanedDf)\n","\n","trainingData, testData = classifierData.randomSplit([0.75, 0.25], seed=42)\n","trainingData.persist()\n","testData.persist()"]},{"cell_type":"markdown","metadata":{},"source":["## Decision Tree Classifier"]},{"cell_type":"markdown","metadata":{},"source":["Create a Decision tree classifier using arbitrary hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dtClassifier = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Severity\", maxDepth=6, minInstancesPerNode=2, maxBins=32)\n","dtModel = dtClassifier.fit(trainingData)\n","testPredictions = dtModel.transform(testData)\n","\n","accuracyEvaluator = MulticlassClassificationEvaluator(labelCol=\"Severity\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = accuracyEvaluator.evaluate(testPredictions)\n","print(f\"Accuracy: {accuracy}\")\n","\n","f1Evaluator = MulticlassClassificationEvaluator(labelCol=\"Severity\", predictionCol=\"prediction\", metricName=\"f1\")\n","f1 = f1Evaluator.evaluate(testPredictions)\n","print(f\"F1: {f1}\")"]},{"cell_type":"markdown","metadata":{},"source":["We perform hyperparameter tuning on the classifier to identify the best set of hyperparameters."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dtHyperparamsEvaluationDf = pd.DataFrame(columns = [\"max depth\", \"min instances per node\", \"training f1\", \"testing f1\", \"training accuracy\", \"testing accuracy\", \"best model\"])\n","\n","index = 0\n","bestIndex = 0\n","highestTestF1 = 0\n","bestDtModel = None\n","\n","maxDepthList = range(2, 10)\n","minInstancesPerNodeList = range(2,15)\n","\n","for maxDepth in maxDepthList:\n","    for minInstances in minInstancesPerNodeList:\n","        seed = 42\n","        dtClassifier = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Severity\", maxDepth=maxDepth, minInstancesPerNode=minInstances, seed=seed)\n","        dtModel = dtClassifier.fit(trainingData)\n","        trainingPredictions = dtModel.transform(trainingData)\n","        testPredictions = dtModel.transform(testData)\n","        trainingF1 = f1Evaluator.evaluate(trainingPredictions)\n","        testingF1 = f1Evaluator.evaluate(testPredictions)\n","        trainingAccuracy = accuracyEvaluator.evaluate(trainingPredictions)\n","        testingAccuracy = accuracyEvaluator.evaluate(testPredictions)\n","\n","        dtHyperparamsEvaluationDf.loc[index] = [maxDepth, minInstances, trainingF1, testingF1, trainingAccuracy, testingAccuracy, 0]\n","\n","        if testingF1 > highestTestF1:\n","            highestTestF1 = testingF1\n","            bestIndex = index\n","            bestDtModel = dtModel\n","\n","        index += 1\n","\n","dtHyperparamsEvaluationDf.loc[bestIndex, \"best model\"] = 1\n","print(dtHyperparamsEvaluationDf.loc[bestIndex])"]},{"cell_type":"markdown","metadata":{},"source":["Our classifier achieved an f1 score on validation data of 0.85 with 85% accuracy. Let's visualize our decision tree to see if we can better understand how it is making its predictions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from decision_tree_plot.decision_tree_parser import decision_tree_parse\n","try:\n","    from decision_tree_plot.decision_tree_plot import plot_trees\n","except ModuleNotFoundError:\n","    !pip3 install jinja2\n","    from decision_tree_plot.decision_tree_plot import plot_trees"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bestModelPath = \"bestDecisionTreeModel\"\n","bestTree = decision_tree_parse(bestDtModel, ss, bestModelPath)\n","column = dict([(str(idx), i) for idx, i in enumerate(classifierData.columns)])\n","plot_trees(bestTree, column = column, output_path = 'decisionTreeVisualization.html')"]},{"cell_type":"markdown","metadata":{},"source":["The visualization of the decision tree is dense and hard to decipher. Later we will see how many features we need to keep to achieve similar amounts of predictive ability, and the visualization should be easier to understand if that succeeds."]},{"cell_type":"markdown","metadata":{},"source":["## Classification with XGBoost\n","\n","We want to try classification using an XGBoost model, but PySpark does not natively support this. So, we use the SparkXGBClassifier provided by [xgboost](https://github.com/dmlc/xgboost)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["xgbClassifier = SparkXGBClassifier(features_col=\"features\", label_col=\"Severity\", max_depth=5, n_estimators=20, seed=42, learning_rate=0.2)\n","xgbModel = xgbClassifier.fit(trainingData)\n","trainingPredictions = xgbModel.transform(trainingData)\n","testPredictions = xgbModel.transform(testData)\n","\n","accuracy = accuracyEvaluator.evaluate(testPredictions)\n","print(f\"Accuracy: {accuracy}\")\n","\n","f1 = f1Evaluator.evaluate(testPredictions)\n","print(f\"F1: {f1}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["xgbHyperparamsEvaluationDf = pd.DataFrame(columns = [\"max depth\", \"num boosting rounds\", \"learning rate\", \"training f1\", \"testing f1\", \"training accuracy\", \"testing accuracy\", \"best model\"])\n","\n","index = 0\n","bestIndex = 0\n","highestTestF1 = 0\n","bestXgbModel = None\n","\n","maxDepthList = range(2, 10)\n","numBoostingRounds = range(20,140,20)\n","learningRates = [0.01, 0.05, 0.1, 0.15, 0.2, 0.3]\n","\n","for maxDepth in maxDepthList:\n","    for boostingRounds in numBoostingRounds:\n","        for learningRate in learningRates:\n","            seed=42\n","            xgbClassifier = SparkXGBClassifier(features_col=\"features\", label_col=\"Severity\", max_depth=maxDepth, n_estimators=boostingRounds, seed=seed, learning_rate=learningRate)\n","            xgbModel = xgbClassifier.fit(trainingData)\n","            trainingPredictions = xgbModel.transform(trainingData)\n","            testPredictions = xgbModel.transform(testData)\n","            trainingF1 = f1Evaluator.evaluate(trainingPredictions)\n","            testingF1 = f1Evaluator.evaluate(testPredictions)\n","            trainingAccuracy = accuracyEvaluator.evaluate(trainingPredictions)\n","            testingAccuracy = accuracyEvaluator.evaluate(testPredictions)\n","\n","            xgbHyperparamsEvaluationDf.loc[index] = [maxDepth, boostingRounds, learningRate, trainingF1, testingF1, trainingAccuracy, testingAccuracy, 0]\n","\n","            if testingF1 > highestTestF1:\n","                highestTestF1 = testingF1\n","                bestIndex = index\n","                bestXgbModel = xgbModel\n","\n","            index += 1\n","\n","xgbHyperparamsEvaluationDf.loc[bestIndex, \"best model\"] = 1\n","print(xgbHyperparamsEvaluationDf.loc[bestIndex])"]},{"cell_type":"markdown","metadata":{},"source":["The XGBoost classifier performed substantially better than the decision tree model. Granted, we tuned more hyperparameters here but even when using 2 hyperparameter as we did with the decision tree the results were better. XGBoost supposedly handles imbalanced data better than standard decision trees, and that may be the reason for the improved performance."]},{"cell_type":"markdown","metadata":{},"source":["## Classification with a neural network (multilayer perceptron)\n","\n","Neural Networks are generally outperformed by tree-based methods such as decision trees as xgboost in classifying tabular data, so we would expect that this method of classification would perform worst on our dataset. "]},{"cell_type":"markdown","metadata":{},"source":["First we create an MLP Classifier with arbitrary hidden layer sizes. The input layer must have 25 nodes to ingest all 25 features of our dataset and the output layer must have 4 nodes for each of the 4 severity classes. The hyperparameter we will be tuning here is the hidden layer sizes i.e. the layers of nodes between the input and output layers."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mlpClassifier = MultilayerPerceptronClassifier(featuresCol=\"features\", labelCol=\"Severity\", layers=[25,12,4,16,4,4])\n","mlpModel = mlpClassifier.fit(trainingData)\n","trainingPredictions = mlpModel.transform(trainingData)\n","\n","accuracy = accuracyEvaluator.evaluate(testPredictions)\n","print(f\"Accuracy: {accuracy}\")\n","\n","f1 = f1Evaluator.evaluate(testPredictions)\n","print(f\"F1: {f1}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mlpHyperparamsEvaluationDf = pd.DataFrame(columns = [\"hidden layer sizes\", \"training f1\", \"testing f1\", \"training accuracy\", \"testing accuracy\", \"best model\"])\n","\n","index = 0\n","bestIndex = 0\n","highestTestF1 = 0\n","bestMlpModel = None\n","\n","hiddenLayerSizes = [[10], [20], [30], [10, 10], [10, 20], [10, 30], [20, 10], [20, 20], [20, 30], [30, 10], [30, 20], [30, 30]]\n","\n","for layerSize in hiddenLayerSizes:\n","    seed=42\n","    mlpClassifier = MultilayerPerceptronClassifier(featuresCol=\"features\", labelCol=\"Severity\", layers=[25] + layerSize + [4])\n","    mlpModel = mlpClassifier.fit(trainingData)\n","    trainingPredictions = mlpModel.transform(trainingData)\n","    testPredictions = mlpModel.transform(testData)\n","    trainingF1 = f1Evaluator.evaluate(trainingPredictions)\n","    testingF1 = f1Evaluator.evaluate(testPredictions)\n","    trainingAccuracy = accuracyEvaluator.evaluate(trainingPredictions)\n","    testingAccuracy = accuracyEvaluator.evaluate(testPredictions)\n","\n","    mlpHyperparamsEvaluationDf.loc[index] = [layerSize, trainingF1, testingF1, trainingAccuracy, testingAccuracy, 0]\n","\n","    if testingF1 > highestTestF1:\n","        highestTestF1 = testingF1\n","        bestIndex = index\n","        bestMlpModel = mlpModel\n","\n","    index += 1\n","\n","mlpHyperparamsEvaluationDf.loc[bestIndex, \"best model\"] = 1\n","print(mlpHyperparamsEvaluationDf.loc[bestIndex])"]},{"cell_type":"markdown","metadata":{},"source":["As expected, the mlp classifier performed much worse than either tree-based classifier. With two hidden layers with 20 nodes, the model only achieved an F1 score of 0.6. It is possible that with better tuning of the hidden layer sizes this approach would work better, so we will increase the number of hidden layer sizes when running on ICDS and see if the results improve significantly."]},{"cell_type":"markdown","metadata":{},"source":["## Results of classification\n","\n","After training and tuning three classification models (decision tree, XGBoost, and multilayer perceptron) we can conclude that, of the models examined, the **XGBoost model is the most powerful predictor of car crash severity**, given this specific dataset. Neither the decision tree nor the multilayer perceptron classifier was able to achieve an F1 score >90% on the test data."]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["#ss.stop()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":2}
