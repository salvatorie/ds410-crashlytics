{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Crashlytics"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["try:\n","    import pyspark\n","except ModuleNotFoundError:\n","    !pip3 install pyspark\n","    import pyspark\n","try:\n","    import pandas as pd\n","except ModuleNotFoundError:\n","    !pip3 install pandas\n","    import pandas as pd\n","    import csv"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import *\n","from pyspark.ml import Pipeline\n","from pyspark.ml.classification import DecisionTreeClassifier\n","from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, IndexToString\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["#May need to install java for this to work\n","ss=SparkSession.builder.master(\"local\").appName(\"crashlytics\").getOrCreate()"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["raw_df = ss.read.csv(\"smallest_crash_data.csv\", header=True, inferSchema=True)"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["#raw_df.printSchema()\n","#raw_df.show(5)"]},{"cell_type":"markdown","metadata":{},"source":["## Imbalance in Data\n","\n","Crash severity of 2 or 3 is much more common than any other severity ranking, and crash durations (duration of impact on traffic) are recorded as 30 minutes the majority of the time. This shows that the data is imbalanced, which we will have to consider when building our classifier."]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-----+\n","|Severity|count|\n","+--------+-----+\n","|       1|    1|\n","|       3|   81|\n","|       2|  118|\n","+--------+-----+\n","\n","+------------------+-----+\n","|         time_diff|count|\n","+------------------+-----+\n","|             -30.0|  151|\n","|             -45.0|   21|\n","|             -60.0|    2|\n","|            -235.0|    1|\n","|            -826.0|    1|\n","|-74.61666666666666|    1|\n","|-64.53333333333333|    1|\n","|            -314.0|    1|\n","|            -137.0|    1|\n","|            -198.0|    1|\n","|-72.98333333333333|    1|\n","|             -92.0|    1|\n","|            -350.0|    1|\n","|            -177.0|    1|\n","|            -109.0|    1|\n","|            -227.0|    1|\n","|             -72.0|    1|\n","|            -871.0|    1|\n","|            -186.0|    1|\n","|            -156.0|    1|\n","+------------------+-----+\n","only showing top 20 rows\n","\n"]}],"source":["severity_count = raw_df.groupBy(F.col(\"Severity\")).count()\n","severity_count.show()\n","\n","time_count = raw_df.withColumn(\"time_diff\", (F.col(\"Start_Time\").cast(\"long\") - F.col(\"End_Time\").cast(\"long\"))/60).groupBy(F.col(\"time_diff\")).count()\n","time_count.orderBy(F.col(\"count\"), ascending=False).show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":2}
